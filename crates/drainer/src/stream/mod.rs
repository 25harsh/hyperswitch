use std::collections::HashMap;

use error_stack::{IntoReport, ResultExt};
use redis_interface as redis;
use router_env::{logger, tracing};
use std::sync::Arc;

use crate::{errors, metrics, query::ExecuteQuery, utils::partition_number, Store, StreamData};

pub type StreamEntries = Vec<(String, StreamEntry)>;

pub type StreamEntry = HashMap<String, String>;

pub type StreamReadResult = HashMap<String, StreamEntries>;

#[derive(Clone)]
pub(crate) struct DrainerStream(pub(crate) Arc<Store>);

#[derive(Clone)]
pub(crate) struct DrainerErrorStream(pub(crate) Arc<Store>);

trait StreamEntryExt {
    fn global_id(&self) -> errors::DrainerResult<String>;
}

impl StreamEntryExt for StreamEntry {
    fn global_id(&self) -> errors::DrainerResult<String> {
        self.get("global_id")
            .ok_or(errors::DrainerError::UnexpectedError(
                "Cannot find global id in the entry".to_string(),
            ))
            .into_report()
            .cloned()
    }
}

impl DrainerStream {
    pub async fn push_to_error_stream(
        &self,
        global_id: &str,
        data: StreamEntry,
    ) -> errors::DrainerResult<()> {
        let conn = &self.store().redis_conn;

        let stream_index = u8::try_from(partition_number(
            global_id,
            self.store().config.drainer_num_partitions,
        ))
        .into_report()
        .change_context(errors::DrainerError::UnexpectedError(
            "Cannot convert u32 to u8 in stream index".to_string(),
        ))?;

        let stream_name = self.get_drainer_error_stream_name(stream_index);

        conn.stream_append_entry(
            &stream_name,
            &redis_interface::RedisEntryId::AutoGeneratedID,
            data.into_iter().collect::<Vec<(String, String)>>(),
        )
        .await
        .map_err(|err| err.into())
        .into_report()?;

        conn.increment_key(&format!("err_{}_in_use", global_id))
            .await
            .map_err(|err| err.into())
            .into_report()?;
        Ok(())
    }
    pub async fn is_locked_for_error(&self, global_id: &str) -> errors::DrainerResult<bool> {
        let conn = &self.store().redis_conn;
        let is_locked = conn
            .get_and_deserialize_key::<u32>(global_id, "u32")
            .await
            .map_err(|err| err.into())
            .into_report()?;
        Ok(is_locked != 0)
    }
}

#[async_trait::async_trait]
pub(crate) trait StreamOperation {
    fn store(&self) -> &Arc<Store>;

    #[inline(always)]
    fn drainer_stream(&self, shard_key: &str) -> String {
        // Example: {shard_5}_drainer_stream
        format!(
            "{{{}}}_{}",
            shard_key,
            self.store().config.drainer_stream_name,
        )
    }

    #[inline(always)]
    fn drainer_error_stream(&self, shard_key: &str) -> String {
        // Example: {shard_5}_drainer_stream
        format!(
            "{{{}}}_{}",
            shard_key,
            self.store().config.drainer_error_stream_name,
        )
    }

    #[inline(always)]
    fn get_stream_key_flag(&self, stream_index: u8) -> String {
        format!("{}_in_use", self.get_drainer_stream_name(stream_index))
    }

    #[inline(always)]
    fn get_drainer_stream_name(&self, stream_index: u8) -> String {
        self.drainer_stream(format!("shard_{stream_index}").as_str())
    }
    #[inline(always)]
    fn get_drainer_error_stream_name(&self, stream_index: u8) -> String {
        self.drainer_error_stream(format!("shard_{stream_index}").as_str())
    }

    #[router_env::instrument(skip_all)]
    async fn is_stream_available(&self, stream_index: u8) -> bool {
        let stream_key_flag = self.get_stream_key_flag(stream_index);

        match self
            .store()
            .redis_conn
            .set_key_if_not_exists_with_expiry(stream_key_flag.as_str(), true, None)
            .await
        {
            Ok(resp) => resp == redis::types::SetnxReply::KeySet,
            Err(error) => {
                logger::error!(operation="lock_stream",err=?error);
                false
            }
        }
    }

    async fn make_stream_available(&self, stream_name_flag: &str) -> errors::DrainerResult<()> {
        match self.store().redis_conn.delete_key(stream_name_flag).await {
            Ok(redis::DelReply::KeyDeleted) => Ok(()),
            Ok(redis::DelReply::KeyNotDeleted) => {
                logger::error!("Tried to unlock a stream which is already unlocked");
                Ok(())
            }
            Err(error) => Err(errors::DrainerError::from(error).into()),
        }
    }

    async fn read_from_stream(
        &self,
        stream_name: &str,
        max_read_count: u64,
    ) -> errors::DrainerResult<StreamReadResult> {
        // "0-0" id gives first entry
        let stream_id = "0-0";
        let (output, execution_time) = common_utils::date_time::time_it(|| async {
            self.store()
                .redis_conn
                .stream_read_entries(stream_name, stream_id, Some(max_read_count))
                .await
                .map_err(errors::DrainerError::from)
                .into_report()
        })
        .await;

        metrics::REDIS_STREAM_READ_TIME.record(
            &metrics::CONTEXT,
            execution_time,
            &[metrics::KeyValue::new("stream", stream_name.to_owned())],
        );

        output
    }
    async fn trim_from_stream(
        &self,
        stream_name: &str,
        minimum_entry_id: &str,
    ) -> errors::DrainerResult<usize> {
        let trim_kind = redis::StreamCapKind::MinID;
        let trim_type = redis::StreamCapTrim::Exact;
        let trim_id = minimum_entry_id;
        let (trim_result, execution_time) =
            common_utils::date_time::time_it::<errors::DrainerResult<_>, _, _>(|| async {
                let trim_result = self
                    .store()
                    .redis_conn
                    .stream_trim_entries(stream_name, (trim_kind, trim_type, trim_id))
                    .await
                    .map_err(errors::DrainerError::from)
                    .into_report()?;

                // Since xtrim deletes entries below given id excluding the given id.
                // Hence, deleting the minimum entry id
                self.store()
                    .redis_conn
                    .stream_delete_entries(stream_name, minimum_entry_id)
                    .await
                    .map_err(errors::DrainerError::from)
                    .into_report()?;

                Ok(trim_result)
            })
            .await;

        metrics::REDIS_STREAM_TRIM_TIME.record(
            &metrics::CONTEXT,
            execution_time,
            &[metrics::KeyValue::new("stream", stream_name.to_owned())],
        );

        // adding 1 because we are deleting the given id too
        Ok(trim_result? + 1)
    }
}

impl StreamOperation for DrainerStream {
    #[inline(always)]
    fn store(&self) -> &Arc<Store> {
        &self.0
    }
}

impl StreamOperation for DrainerErrorStream {
    #[inline(always)]
    fn store(&self) -> &Arc<Store> {
        &self.0
    }
}

#[async_trait::async_trait]
pub(crate) trait DrainStream {
    async fn drain_stream(
        data: &StreamEntries,
        stream: Self,
        stream_name: &str,
    ) -> errors::DrainerResult<()>;
}

#[async_trait::async_trait]
impl DrainStream for DrainerStream {
    async fn drain_stream(
        data: &StreamEntries,
        stream: Self,
        stream_name: &str,
    ) -> errors::DrainerResult<()> {
        let session_id = common_utils::generate_id_with_default_len("drainer_session");

        let read_count = data.len();

        let id_to_trim = &data
            .last()
            .ok_or(error_stack::report!(errors::DrainerError::UnexpectedError(
                "Empty stream".to_string()
            )))?
            .0;

        let drain_proc = |entry_id: String, entry: HashMap<String, String>| async {
            let data = StreamData::from_hashmap(entry)
                .map_err(|err| {
                    logger::error!(operation = "deserialization", err=?err);
                    metrics::STREAM_PARSE_FAIL.add(
                        &metrics::CONTEXT,
                        1,
                        &[metrics::KeyValue {
                            key: "operation".into(),
                            value: "deserialization".into(),
                        }],
                    );
                    errors::DrainerError::ParsingError(err)
                })
                .into_report()?;

            tracing::Span::current().record("request_id", data.request_id);
            tracing::Span::current().record("global_id", data.global_id);
            tracing::Span::current().record("session_id", &session_id);

            data.typed_sql
                .execute_query(stream.store(), data.pushed_at)
                .await
                .map_err(|err| error_stack::report!(errors::DrainerError::DatabaseError(err)))?;

            Ok::<_, error_stack::Report<errors::DrainerError>>(())
        };

        for (entry_id, entry) in data.clone() {
            let global_id = entry.global_id()?;
            if !stream.is_locked_for_error(&global_id).await? {
                stream
                    .push_to_error_stream(&global_id, entry.clone())
                    .await?;
            }

            let result = drain_proc(entry_id, entry.clone()).await;
            match result {
                Err(_) => {
                    stream
                        .push_to_error_stream(&global_id, entry.clone())
                        .await?;
                }
                Ok(_) => {}
            }
        }

        let entries_trimmed = stream.trim_from_stream(stream_name, &id_to_trim).await?;

        if read_count != entries_trimmed {
            logger::error!(
                read_entries = %read_count,
                trimmed_entries = %entries_trimmed,
                ?data,
                "Assertion Failed no. of entries read from the stream doesn't match no. of entries trimmed"
            );
        }

        Ok(())
    }
}

#[async_trait::async_trait]
impl DrainStream for DrainerErrorStream {
    async fn drain_stream(
        data: &StreamEntries,
        stream: Self,
        stream_name: &str,
    ) -> errors::DrainerResult<()> {
        let session_id = common_utils::generate_id_with_default_len("drainer_session");

        let read_count = data.len();
        let mut last_processed_id = String::new();

        for (entry_id, entry) in data.clone() {
            let data = match StreamData::from_hashmap(entry) {
                Ok(data) => data,
                Err(err) => {
                    logger::error!(operation = "deserialization", err=?err);
                    metrics::STREAM_PARSE_FAIL.add(
                        &metrics::CONTEXT,
                        1,
                        &[metrics::KeyValue {
                            key: "operation".into(),
                            value: "deserialization".into(),
                        }],
                    );

                    // break from the loop in case of a deser error
                    break;
                }
            };

            tracing::Span::current().record("request_id", &data.request_id);
            tracing::Span::current().record("global_id", &data.global_id);
            tracing::Span::current().record("session_id", &session_id);

            match data
                .typed_sql
                .execute_query(stream.store(), data.pushed_at)
                .await
            {
                Ok(_) => {
                    last_processed_id = entry_id;
                }
                Err(err) => match err.current_context() {
                    // In case of Uniqueviolation we can't really do anything to fix it so just clear
                    // it from the stream
                    diesel_models::errors::DatabaseError::UniqueViolation => {
                        last_processed_id = entry_id;
                    }
                    // break from the loop in case of an error in query
                    _ => break,
                },
            }
            stream
                .store()
                .redis_conn
                .decrement_key(&format!("err_{}_in_use", data.global_id))
                .await
                .map_err(|err| error_stack::report!(errors::DrainerError::RedisError(err)))?;
        }
        if !last_processed_id.is_empty() {
            let entries_trimmed = stream
                .trim_from_stream(stream_name, &last_processed_id)
                .await?;
            if read_count != entries_trimmed {
                logger::error!(
                    read_entries = %read_count,
                    trimmed_entries = %entries_trimmed,
                    ?data,
                    "Assertion Failed no. of entries read from the stream doesn't match no. of entries trimmed"
                );
            }
        } else {
            logger::error!(read_entries = %read_count,"No streams were processed in this session");
        }

        Ok(())
    }
}
